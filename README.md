# üìä Ingest√£o de Dados de Bitcoin no Databricks

Este projeto demonstra, o passo a passo de um **pipeline simples de ingest√£o de dados** utilizando **Databricks**, consumindo dados de APIs externas, tratando as informa√ß√µes e persistindo os resultados em uma **tabela Delta Lake**. Ao final, o notebook √© executado de forma **automatizada e agendada** por meio de um **Databricks Job**.

---

## üß† Vis√£o Geral do Processo

O fluxo executado no Databricks segue as etapas abaixo:

1. Importa√ß√£o das bibliotecas necess√°rias
2. Cria√ß√£o de um par√¢metro (widget) para receber a API Key
3. Extra√ß√£o de dados da API do Coinbase (Bitcoin)
4. Extra√ß√£o da taxa de c√¢mbio USD ‚Üí BRL via API CurrencyFreaks
5. Tratamento e enriquecimento dos dados
6. Cria√ß√£o de cat√°logo e schema no Databricks
7. Cria√ß√£o de um DataFrame Spark
8. Persist√™ncia dos dados em uma tabela Delta
9. Cria√ß√£o de um Job para execu√ß√£o agendada do notebook

---

## 1Ô∏è‚É£ Importa√ß√£o das bibliotecas

Inicialmente, s√£o importadas as bibliotecas necess√°rias para requisi√ß√µes HTTP, manipula√ß√£o de dados e gera√ß√£o de timestamps.

```python
import requests
import pandas as pd
from datetime import datetime
```

---

## 2Ô∏è‚É£ Cria√ß√£o de par√¢metro (Widget)

No Databricks, foi criado um **widget de texto** para receber a **API Key da CurrencyFreaks** dinamicamente, sem necessidade de alterar o c√≥digo.

```python
dbutils.widgets.text("api_key", "", "API Key CurrencyFreaks")
```

üìå **Objetivo:** permitir que o valor da API Key seja informado manualmente ou passado automaticamente por um Job.

---

## 3Ô∏è‚É£ Extra√ß√£o de dados da API Coinbase

Foi criada uma fun√ß√£o respons√°vel por buscar o valor atual do Bitcoin em USD a partir da API p√∫blica do Coinbase.

```python
def extrair_dados_bitcoin():
    url = 'https://api.coinbase.com/v2/prices/spot'
    resultado = requests.get(url)
    return resultado.json()
```

Em seguida, os dados s√£o atribu√≠dos a uma vari√°vel:

```python
dados_bitcoin = extrair_dados_bitcoin()
```

---

## 4Ô∏è‚É£ Extra√ß√£o da taxa de c√¢mbio (CurrencyFreaks)

Outra fun√ß√£o foi criada para consumir a API da **CurrencyFreaks**, utilizando a API Key informada no widget.

```python
def extrair_dados_currentfreaks():
    api_key = dbutils.widgets.get("api_key")
    url = f'https://api.currencyfreaks.com/v2.0/rates/latest?apikey={api_key}'
    resultado = requests.get(url)
    return resultado.json()
```

Ap√≥s a extra√ß√£o, √© obtida apenas a taxa de convers√£o de USD para BRL:

```python
dados_cotacao = extrair_dados_currentfreaks()
cotacao_br = float(dados_cotacao['rates']['BRL'])
```

---

## 5Ô∏è‚É£ Tratamento e enriquecimento dos dados

Nesta etapa, os dados das duas APIs s√£o combinados e tratados em uma √∫nica estrutura, incluindo:

* Valor do Bitcoin em USD
* Moeda base
* Moeda de origem
* Convers√£o para BRL
* Timestamp da extra√ß√£o

```python
def tratar_dados_bitcoin(dados_bitcoin, cotacao_br):
    valor_usd = float(dados_bitcoin['data']['amount'])
    criptomoeada = dados_bitcoin['data']['base']
    moeada_orig = dados_bitcoin['data']['currency']

    valor_br1 = valor_usd * cotacao_br
    timestamp = datetime.now()

    dados_tratados = [{
        "valor_usd": valor_usd,
        "criptomoeada": criptomoeada,
        "moeada_orig": moeada_orig,
        "taxa_conversao_usd_to_brl": valor_br1,
        "timestamp": timestamp
    }]
    return dados_tratados
```

---

## 6Ô∏è‚É£ Cria√ß√£o do DataFrame Spark

Os dados tratados s√£o convertidos para um **DataFrame Spark**, permitindo integra√ß√£o com o Delta Lake.

```python
df_dados_bitcoin = tratar_dados_bitcoin(dados_bitcoin, cotacao_br)
df = spark.createDataFrame(df_dados_bitcoin)
```

---

## 7Ô∏è‚É£ Cria√ß√£o do Cat√°logo e Schema

Utilizando comandos SQL no Databricks, foi criado um **cat√°logo** e um **schema** para organiza√ß√£o dos dados.

```sql
CREATE CATALOG IF NOT EXISTS catalogo_bitcoin;
```

```sql
CREATE SCHEMA IF NOT EXISTS catalogo_bitcoin.data_bitcoin;
```

---

## 8Ô∏è‚É£ Persist√™ncia dos dados em Delta Lake

Os dados s√£o gravados em uma **tabela Delta**, permitindo escalabilidade, versionamento e integra√ß√£o com outros pipelines.

```python
create_table_delta_name = 'catalogo_bitcoin.data_bitcoin.bitcoin_data'

(df.write
  .format("delta")
  .mode("append")
  .option("mergeSchema", "True")
  .saveAsTable(create_table_delta_name)
)
```

üìå **Observa√ß√£o:** o modo `append` permite inserir novos registros a cada execu√ß√£o do pipeline.

---

## 9Ô∏è‚É£ Cria√ß√£o de Job e execu√ß√£o agendada

Por fim, foi criado um **Databricks Job** para executar este notebook de forma **autom√°tica e agendada**, sem interven√ß√£o manual.

### ‚úî O Job permite:

* Executar o notebook em intervalos definidos (ex: a cada 5 ou 20 minutos)
* Passar o valor da `api_key` como par√¢metro
* Automatizar completamente o processo de ingest√£o

üìÖ O agendamento foi configurado utilizando **cron no padr√£o Databricks (Quartz)**.

---

## ‚úÖ Conclus√£o

Este notebook demonstra um fluxo completo de:

* Consumo de APIs externas
* Parametriza√ß√£o no Databricks
* Tratamento de dados
* Persist√™ncia em Delta Lake
* Automa√ß√£o com Jobs agendados

Tudo isso foi realizado **inteiramente dentro do Databricks**, seguindo boas pr√°ticas de engenharia de dados.
